{'cross-native-lang-tm': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 3], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'cross-native-lang-mt': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-tm': {'model1': {'ngram_range': [1, 2], 'analyzer': 'word', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-mt': {'model1': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'within-native-lang-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}, 'within-native-lang-medium': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-medium': {'model1': {'ngram_range': [1, 7], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}}
{'memory': None, 'steps': [('vect', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 5], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)), ('clf', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False))], 'vect': TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 5], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'clf': SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False), 'vect__analyzer': 'char', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': False, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': [1, 5], 'vect__norm': 'l2', 'vect__preprocessor': None, 'vect__smooth_idf': True, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__sublinear_tf': False, 'vect__token_pattern': '(?u)\\b\\w\\w+\\b', 'vect__tokenizer': None, 'vect__use_idf': True, 'vect__vocabulary': None, 'clf__C': 1, 'clf__cache_size': 200, 'clf__class_weight': None, 'clf__coef0': 0.0, 'clf__decision_function_shape': 'ovr', 'clf__degree': 3, 'clf__gamma': 'auto', 'clf__kernel': 'linear', 'clf__max_iter': -1, 'clf__probability': True, 'clf__random_state': None, 'clf__shrinking': True, 'clf__tol': 0.001, 'clf__verbose': False}
18
Stacking Ngrams + pos...
Predictions stacked for file ./my_data2/data_final_cross_genre_native_lang_tm.csv
0.23427331887201736
{'cross-native-lang-tm': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 3], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'cross-native-lang-mt': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-tm': {'model1': {'ngram_range': [1, 2], 'analyzer': 'word', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-mt': {'model1': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'within-native-lang-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}, 'within-native-lang-medium': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-medium': {'model1': {'ngram_range': [1, 7], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}}
{'memory': None, 'steps': [('vect', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 5], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)), ('clf', SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False))], 'vect': TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 5], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'clf': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False), 'vect__analyzer': 'char', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': False, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': [1, 5], 'vect__norm': 'l2', 'vect__preprocessor': None, 'vect__smooth_idf': True, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__sublinear_tf': False, 'vect__token_pattern': '(?u)\\b\\w\\w+\\b', 'vect__tokenizer': None, 'vect__use_idf': True, 'vect__vocabulary': None, 'clf__C': 10, 'clf__cache_size': 200, 'clf__class_weight': None, 'clf__coef0': 0.0, 'clf__decision_function_shape': 'ovr', 'clf__degree': 3, 'clf__gamma': 'auto', 'clf__kernel': 'linear', 'clf__max_iter': -1, 'clf__probability': True, 'clf__random_state': None, 'clf__shrinking': True, 'clf__tol': 0.001, 'clf__verbose': False}
18
Stacking Ngrams + pos...
Predictions stacked for file ./my_data2/data_final_cross_genre_native_lang_mt.csv
0.26440677966101694
{'cross-native-lang-tm': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 3], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'cross-native-lang-mt': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-tm': {'model1': {'ngram_range': [1, 2], 'analyzer': 'word', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-mt': {'model1': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'within-native-lang-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}, 'within-native-lang-medium': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-medium': {'model1': {'ngram_range': [1, 7], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}}
{'memory': None, 'steps': [('vect', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 4], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)), ('clf', SVC(C=20, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False))], 'vect': TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 4], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'clf': SVC(C=20, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False), 'vect__analyzer': 'char', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': False, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': [1, 4], 'vect__norm': 'l2', 'vect__preprocessor': None, 'vect__smooth_idf': True, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__sublinear_tf': False, 'vect__token_pattern': '(?u)\\b\\w\\w+\\b', 'vect__tokenizer': None, 'vect__use_idf': True, 'vect__vocabulary': None, 'clf__C': 20, 'clf__cache_size': 200, 'clf__class_weight': None, 'clf__coef0': 0.0, 'clf__decision_function_shape': 'ovr', 'clf__degree': 3, 'clf__gamma': 'auto', 'clf__kernel': 'linear', 'clf__max_iter': -1, 'clf__probability': True, 'clf__random_state': None, 'clf__shrinking': True, 'clf__tol': 0.001, 'clf__verbose': False}
18
Stacking Ngrams + pos...
Predictions stacked for file ./my_data2/data_final_within_genre_native_lang_twitter.csv
0.856655290102389
{'cross-native-lang-tm': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 3], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'cross-native-lang-mt': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-tm': {'model1': {'ngram_range': [1, 2], 'analyzer': 'word', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-mt': {'model1': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'within-native-lang-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}, 'within-native-lang-medium': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-medium': {'model1': {'ngram_range': [1, 7], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}}
{'memory': None, 'steps': [('vect', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 5], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)), ('clf', SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False))], 'vect': TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 5], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'clf': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False), 'vect__analyzer': 'char', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': False, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': [1, 5], 'vect__norm': 'l2', 'vect__preprocessor': None, 'vect__smooth_idf': True, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__sublinear_tf': False, 'vect__token_pattern': '(?u)\\b\\w\\w+\\b', 'vect__tokenizer': None, 'vect__use_idf': True, 'vect__vocabulary': None, 'clf__C': 10, 'clf__cache_size': 200, 'clf__class_weight': None, 'clf__coef0': 0.0, 'clf__decision_function_shape': 'ovr', 'clf__degree': 3, 'clf__gamma': 'auto', 'clf__kernel': 'linear', 'clf__max_iter': -1, 'clf__probability': True, 'clf__random_state': None, 'clf__shrinking': True, 'clf__tol': 0.001, 'clf__verbose': False}
18
Stacking Ngrams + pos...
Predictions stacked for file ./my_data2/data_final_within_genre_native_lang_medium.csv
0.8491803278688524
{'cross-native-lang-tm': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 3], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'cross-native-lang-mt': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-tm': {'model1': {'ngram_range': [1, 2], 'analyzer': 'word', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-mt': {'model1': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'within-native-lang-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}, 'within-native-lang-medium': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-medium': {'model1': {'ngram_range': [1, 7], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}}
{'memory': None, 'steps': [('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 2], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)), ('clf', SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False))], 'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 2], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'clf': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': False, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': [1, 2], 'vect__norm': 'l2', 'vect__preprocessor': None, 'vect__smooth_idf': True, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__sublinear_tf': False, 'vect__token_pattern': '(?u)\\b\\w\\w+\\b', 'vect__tokenizer': None, 'vect__use_idf': True, 'vect__vocabulary': None, 'clf__C': 10, 'clf__cache_size': 200, 'clf__class_weight': None, 'clf__coef0': 0.0, 'clf__decision_function_shape': 'ovr', 'clf__degree': 3, 'clf__gamma': 'auto', 'clf__kernel': 'linear', 'clf__max_iter': -1, 'clf__probability': True, 'clf__random_state': None, 'clf__shrinking': True, 'clf__tol': 0.001, 'clf__verbose': False}
4
Stacking Ngrams + pos...
Predictions stacked for file ./my_data2/data_final_cross_genre_gender_tm.csv
0.6409978308026031
{'cross-native-lang-tm': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 3], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'cross-native-lang-mt': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-tm': {'model1': {'ngram_range': [1, 2], 'analyzer': 'word', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-mt': {'model1': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'within-native-lang-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}, 'within-native-lang-medium': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-medium': {'model1': {'ngram_range': [1, 7], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}}
{'memory': None, 'steps': [('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 1], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)), ('clf', SVC(C=20, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False))], 'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 1], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'clf': SVC(C=20, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': False, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': [1, 1], 'vect__norm': 'l2', 'vect__preprocessor': None, 'vect__smooth_idf': True, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__sublinear_tf': False, 'vect__token_pattern': '(?u)\\b\\w\\w+\\b', 'vect__tokenizer': None, 'vect__use_idf': True, 'vect__vocabulary': None, 'clf__C': 20, 'clf__cache_size': 200, 'clf__class_weight': None, 'clf__coef0': 0.0, 'clf__decision_function_shape': 'ovr', 'clf__degree': 3, 'clf__gamma': 'auto', 'clf__kernel': 'linear', 'clf__max_iter': -1, 'clf__probability': True, 'clf__random_state': None, 'clf__shrinking': True, 'clf__tol': 0.001, 'clf__verbose': False}
4
Stacking Ngrams + pos...
Predictions stacked for file ./my_data2/data_final_cross_genre_gender_mt.csv
0.632768361581921
{'cross-native-lang-tm': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 3], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'cross-native-lang-mt': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-tm': {'model1': {'ngram_range': [1, 2], 'analyzer': 'word', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-mt': {'model1': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'within-native-lang-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}, 'within-native-lang-medium': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-medium': {'model1': {'ngram_range': [1, 7], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}}
{'memory': None, 'steps': [('vect', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 4], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)), ('clf', SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False))], 'vect': TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 4], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'clf': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False), 'vect__analyzer': 'char', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': False, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': [1, 4], 'vect__norm': 'l2', 'vect__preprocessor': None, 'vect__smooth_idf': True, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__sublinear_tf': False, 'vect__token_pattern': '(?u)\\b\\w\\w+\\b', 'vect__tokenizer': None, 'vect__use_idf': True, 'vect__vocabulary': None, 'clf__C': 10, 'clf__cache_size': 200, 'clf__class_weight': None, 'clf__coef0': 0.0, 'clf__decision_function_shape': 'ovr', 'clf__degree': 3, 'clf__gamma': 'auto', 'clf__kernel': 'linear', 'clf__max_iter': -1, 'clf__probability': True, 'clf__random_state': None, 'clf__shrinking': True, 'clf__tol': 0.001, 'clf__verbose': False}
4
Stacking Ngrams + pos...
Predictions stacked for file ./my_data2/data_final_within_genre_gender_twitter.csv
0.9351535836177475
{'cross-native-lang-tm': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 3], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'cross-native-lang-mt': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-tm': {'model1': {'ngram_range': [1, 2], 'analyzer': 'word', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 1, 'kernel': 'linear'}}, 'cross-gender-mt': {'model1': {'ngram_range': [1, 1], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}}, 'within-native-lang-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 5], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}, 'within-native-lang-medium': {'model1': {'ngram_range': [1, 5], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-twitter': {'model1': {'ngram_range': [1, 4], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 8], 'analyzer': 'char', 'C': 20, 'kernel': 'linear'}}, 'within-gender-medium': {'model1': {'ngram_range': [1, 7], 'analyzer': 'char', 'C': 10, 'kernel': 'linear'}, 'model2': {'ngram_range': [1, 4], 'analyzer': 'word', 'C': 20, 'kernel': 'linear'}}}
{'memory': None, 'steps': [('vect', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 7], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)), ('clf', SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False))], 'vect': TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=False, max_df=1.0, max_features=None, min_df=1,
        ngram_range=[1, 7], norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'clf': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False), 'vect__analyzer': 'char', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': False, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': [1, 7], 'vect__norm': 'l2', 'vect__preprocessor': None, 'vect__smooth_idf': True, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__sublinear_tf': False, 'vect__token_pattern': '(?u)\\b\\w\\w+\\b', 'vect__tokenizer': None, 'vect__use_idf': True, 'vect__vocabulary': None, 'clf__C': 10, 'clf__cache_size': 200, 'clf__class_weight': None, 'clf__coef0': 0.0, 'clf__decision_function_shape': 'ovr', 'clf__degree': 3, 'clf__gamma': 'auto', 'clf__kernel': 'linear', 'clf__max_iter': -1, 'clf__probability': True, 'clf__random_state': None, 'clf__shrinking': True, 'clf__tol': 0.001, 'clf__verbose': False}
4
Stacking Ngrams + pos...
Predictions stacked for file ./my_data2/data_final_within_genre_gender_medium.csv
0.8852459016393442


###############################################################################
Peregrine Cluster
Job 1514549 for user 's2380250'
Finished at: Tue Jun 26 23:31:25 CEST 2018

Job details:
============

Name                : stacking_job_pos.sh
User                : s2380250
Partition           : gpu
Nodes               : pg-gpu03
Cores               : 12
State               : COMPLETED
Submit              : 2018-06-26T16:26:23
Start               : 2018-06-26T19:40:27
End                 : 2018-06-26T23:31:25
Reserved walltime   : 04:30:00
Used walltime       : 03:50:58
Used CPU time       : 03:50:53 (efficiency:  8.33%)
% User (Computation): 99.92%
% System (I/O)      :  0.08%
Mem reserved        : 8000M/node
Max Mem used        : 745.12M (pg-gpu03)
Max Disk Write      : 4.89M (pg-gpu03)
Max Disk Read       : 219.01M (pg-gpu03)


Acknowledgements:
=================

Please see this page if you want to acknowledge Peregrine in your publications:

https://redmine.hpc.rug.nl/redmine/projects/peregrine/wiki/ScientificOutput

################################################################################
